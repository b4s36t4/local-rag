---
import Layout from "../layouts/Layout.astro";
import { RenderMessageList } from "../components/RenderMessageList.tsx";
import { Paperclip } from "lucide-react";
---

<Layout title="ChatLLM">
  <div
    class="mx-[10%] max-h-[calc(100vh-150px)] mt-10 border-2 flex flex-col rounded-md border-slate-300"
  >
    <RenderMessageList client:load />

    <div class="flex flex-col m-2 relative">
      <div
        class="flex bg-red-100 p-1 controlled-border border-2 border-b-0 rounded-tr-lg rounded-tl-lg"
      >
        <p class="text-sm">
          Model Selected: <span class="font-bold" id="selectedModel"></span>
        </p>
        <button id="loadOrDownload" class="ml-4 p-0 m-0 text-blue-600"></button>
      </div>
      <div
        class="flex border-b-2 border-l-2 border-r-2 overflow-scroll controlled-border rounded-br-lg rounded-bl-lg"
      >
        <textarea
          placeholder="Enter Message"
          id="promptInput"
          class="w-[85%] outline-none p-2 flex-1"
          rows="3"
          style="resize: none;"></textarea>
        <div class="flex mx-4 py-4 items-center space-x-1">
          <button
            ><Paperclip
              className="mr-4"
              role="button"
              client:visible
              id="attachmentPickerBtn"
            />
          </button>

          <input
            type="file"
            accept="text/pdf"
            class="hidden"
            id="attachmentPicker"
          />
          <button
            class="h-full bg-slate-200 mr-4 rounded-lg p-4 my-auto"
            id="sendChat">Send</button
          >
        </div>
      </div>
    </div>
  </div>
</Layout>

<style>
  #statusTooltip {
    width: max-content;
    position: absolute;
    top: 0;
    left: 0;
    background: #222;
    color: white;
    font-weight: bold;
    padding: 5px;
    border-radius: 4px;
    font-size: 90%;
  }

  :root {
    --input-border-color: rgb(55 65 81 / var(--tw-border-opacity));
  }

  .controlled-border {
    border-color: var(--input-border-color);
  }

  .shiki {
    @apply overflow-x-auto rounded-lg p-4;
  }
</style>

<script>
  import {
    hasModelInCache,
    prebuiltAppConfig,
    CreateMLCEngine,
    MLCEngine,
    type InitProgressCallback,
    type ChatCompletionMessageParam,
  } from "@mlc-ai/web-llm";
  import { chatStore, newMessageStore } from "../store/chat";
  import { modelStore } from "../store/modelStore";
  import Toastify from "toastify-js";
  import { toCompletionMessage } from "../utils/message";
  import { WebPDFLoader } from "@langchain/community/document_loaders/web/pdf";
  import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
  import * as pdfjs from "pdfjs-dist";
  import { EmbedDB } from "../embed/db";
  // import { insert, search } from "@orama/orama";

  // https://cdnjs.cloudflare.com/ajax/libs/pdf.js/4.3.136/pdf.worker.mjs

  pdfjs.GlobalWorkerOptions.workerSrc = `//cdnjs.cloudflare.com/ajax/libs/pdf.js/${pdfjs.version}/pdf.worker.mjs`;

  let model: MLCEngine;

  window.MODEL_LOADED = false;
  window.CHAT_PDF = false;

  const { pushMessage } = chatStore.getState();

  const embedWorker = new Worker(
    new URL("../embed/worker.ts", import.meta.url),
    { type: "module" }
  );

  const splitter = new RecursiveCharacterTextSplitter({
    // chunkSize: 512,
    // chunkOverlap: 100,
  });

  const db = await EmbedDB.getDB();
  const attachmentInput = document.getElementById(
    "attachmentPicker"
  ) as HTMLInputElement;
  const attachmentPickerBtn = document.getElementById("attachmentPickerBtn");

  let currentRunningToast: any = null;

  attachmentPickerBtn?.addEventListener("click", () => {
    attachmentInput?.click();
  });

  const askRAGQuestion = async ({
    question,
    context,
  }: {
    question: string;
    context: string;
  }) => {
    const prompt = `
    
    Give the context below answer my questions.

    context: ${context}

    question: ${question}

    `;

    const systemMessage: ChatCompletionMessageParam = {
      role: "user",
      content: prompt,
    };

    const output = await model.chat.completions.create({
      messages: [systemMessage],
      n: 1,
      stream_options: { include_usage: true },
      stream: true,
      // max_tokens: 512,
      temperature: 0.2,
    });

    const message: ChatCompletionMessageParam = {
      content: "",
      role: "assistant",
    };

    newMessageStore.setState({ generating: true });

    for await (const chunk of output) {
      if (
        chunk.choices.length === 0 ||
        chunk.choices[0]?.finish_reason === "stop"
      ) {
        break;
      }

      const chunkMessage = chunk.choices[0].delta.content;
      if (!chunkMessage) {
        break;
      }
      message["content"]! += chunkMessage;

      newMessageStore.setState({ message: message["content"] ?? "" });

      // Indicating that generation has stopped
    }

    pushMessage(message);

    newMessageStore.setState({ generating: false, message: "" });
    enableSend();
  };

  embedWorker.addEventListener("message", async (event) => {
    const isProgressEvent = event.data?.status === "progress";
    const isProgressDone = event.data?.status === "ready";

    if (isProgressEvent && currentRunningToast === null) {
      currentRunningToast = Toastify({
        text: "Loading embed Model...",
        duration: -1,
      });
      currentRunningToast?.showToast();
      return;
    }

    if (isProgressDone) {
      currentRunningToast.hideToast();

      Toastify({ text: "Embedding model loaded!", duration: 1000 }).showToast();
    }

    const isEmbed = event.data.type === "embedDocument";
    if (isEmbed) {
      // console.log(event, "event-data");
      const embeddings = event.data.data;

      if (!db) {
        return;
      }

      // insert(db, {
      //   doucment: event.data.payload,
      //   embedding: embeddings?.at(0),
      // });
      db.add({
        embeddings: [
          {
            id: crypto.randomUUID(),
            embeddings: embeddings?.at(0) as number[],
            title: event.data.payload as string,
            url: "",
          },
        ],
      });
    }

    const isEmbedQuestion = event.data.type === "embedQuestion";
    if (isEmbedQuestion) {
      if (!db) {
        return;
      }
      const embedding = event.data.data?.at(0);

      const similarity = db.search(embedding, 5);

      let context = "";

      similarity.neighbors.forEach((item) => {
        context += item.title + "\n\n";
      });

      // results.hits.forEach((hit) => {
      //   context += hit.document + "\n\n";
      // });


      await askRAGQuestion({ question: event.data.payload, context: context });
    }
  });

  attachmentInput?.addEventListener("change", async function () {
    const file = this.files?.item(0);
    if (!file) {
      return;
    }
    const loader = new WebPDFLoader(file, {
      splitPages: false,
      pdfjs: async () => ({
        getDocument: pdfjs.getDocument,
        version: pdfjs.version,
      }),
    });

    embedWorker.postMessage({ type: "initiate" });

    window.CHAT_PDF = true;

    const documents = await loader.load();

    const splits = await splitter.splitDocuments(documents);

    splits.slice(0, 15).forEach((item) => {
      embedWorker.postMessage({
        data: item.pageContent,
        id: crypto.randomUUID(),
        type: "embedDocument",
      });
    });

    Toastify({ text: "PDF Embedding completed", duration: 1000 }).showToast();

    // currentRunningToast = Toastify({ text: "Loading file" }).showToast();
    // embedWorker.postMessage({ data: ["Hello"] });
  });
  // embedWorker.addEventListener("message", (event) => {
  //   console.log("event", event);
  // });

  // embedWorker.postMessage({
  //   data: ["embed this", "sun is so hot", "moon is not visible"],
  // });

  // Updateable Text fields
  const modelText = document.getElementById("selectedModel");
  const modelStatus = document.getElementById("loadOrDownload");

  const sendButton = document.getElementById("sendChat");
  const input = document.getElementById("promptInput") as HTMLInputElement;

  let selectedModel = null;

  if (!modelStore.getState().selectedModel) {
    // Get the model which requires the least ram, so trying is easy
    selectedModel = prebuiltAppConfig.model_list.sort().at(-1);

    // selectedModel!.overrides = {
    //   sliding_window_size: 1024,
    //   temperature: 0.5,
    //   top_p: 0.3,
    //   attention_sink_size: 4,
    //   frequency_penalty: 0.3,
    //   presence_penalty: 0.5,
    //   repetition_penalty: 0.2,
    // };

    modelStore.setState({ selectedModel: selectedModel });
  } else {
    selectedModel = modelStore.getState().selectedModel;
  }

  selectedModel!.overrides = {
    // sliding_window_size: 1024,
    temperature: 0.5,
    top_p: 0.3,
    attention_sink_size: 4,
    frequency_penalty: 0.3,
    presence_penalty: 0.5,
    repetition_penalty: 0.2,
    context_window_size: 1024,
  };
  // Check weather the model is already downloaded
  const isSelectedModelAvailable = await hasModelInCache(
    selectedModel?.model_id ?? "",
    prebuiltAppConfig
  );

  const checkForGPU = async () => {
    const isGPUAvailable = "gpu" in window.navigator;
    const isGPUAdapoterAvailable =
      isGPUAvailable && (await (window.navigator as any).gpu.requestAdapter());
    if (!isGPUAvailable || !isGPUAdapoterAvailable) {
      sendButton!.textContent = "Not available";
      modelStatus!.innerHTML = "Unavailable";
      return false;
    }

    return true;
  };

  // Disable send button while model loading
  const disableSend = () => {
    if (sendButton) {
      sendButton.setAttribute("disabled", "true");
      sendButton.textContent = "Loading...";
    }
  };

  // enable send button when mode is loaded
  const enableSend = () => {
    if (sendButton) {
      sendButton.removeAttribute("disabled");
      sendButton.textContent = "Send";
    }
  };

  // Callback to handle the progress event
  const handleProgress: InitProgressCallback = function (report) {
    if (report.progress === 1) {
      enableSend();
      modelStatus!.innerText = "Loaded";
      return;
    }
    const progress = Math.round(report.progress * 100);
    if (modelStatus) {
      modelStatus.innerText = `${progress}% of 100%`;
    }
  };

  const startModel = async function () {
    if (window.MODEL_LOADED) {
      return;
    }
    model = await CreateMLCEngine(
      selectedModel?.model_id ?? "",
      {
        logLevel: "DEBUG",
        initProgressCallback: handleProgress,
        appConfig: prebuiltAppConfig,
      },
      {
        sliding_window_size: 512,
        context_window_size: -1,
        // temperature: 0.2,
        attention_sink_size: 4,
        // frequency_penalty: 0.6,
        // presence_penalty: 0.5,
        // repetition_penalty: 0.3,
      }
    );

    window.MODEL_LOADED = true;
  };

  const loadModel = async () => {
    modelStatus!.addEventListener("click", async function (event) {
      event.preventDefault();
      disableSend();

      startModel();
    });

    const resetInputText = async () => {
      input.value = "";
    };

    sendButton?.addEventListener("click", async function () {
      const question = input.value;
      const questionMessage = toCompletionMessage(question);

      disableSend();
      if (question.trim().length === 0) {
        alert("Please ask what you want?!");
        return;
      }
      if (!window.MODEL_LOADED) {
        disableSend();
        await startModel();
        window.MODEL_LOADED = true;
      }

      if (window.CHAT_PDF) {
        if (!db) {
          return;
        }

        embedWorker.postMessage({ type: "embedQuestion", data: question });
        pushMessage(questionMessage);
        resetInputText();
        return;
      }

      pushMessage(questionMessage);
      resetInputText();

      const output = await model.chat.completions.create({
        messages: chatStore.getState().messages,
        n: 1,
        stream_options: { include_usage: true },
        stream: true,
        // max_tokens: 512,
        temperature: 0.2,
      });

      const message: ChatCompletionMessageParam = {
        content: "",
        role: "assistant",
      };

      newMessageStore.setState({ generating: true });

      for await (const chunk of output) {
        if (
          chunk.choices.length === 0 ||
          chunk.choices[0]?.finish_reason === "stop"
        ) {
          break;
        }

        const chunkMessage = chunk.choices[0].delta.content;
        if (!chunkMessage) {
          break;
        }
        message["content"]! += chunkMessage;

        newMessageStore.setState({ message: message["content"] ?? "" });

        // Indicating that generation has stopped
      }

      pushMessage(message);

      newMessageStore.setState({ generating: false, message: "" });
      enableSend();
    });
  };

  if (modelText) {
    modelText.innerText = selectedModel?.model_id ?? "";
  }

  if (modelStatus && (await checkForGPU())) {
    modelStatus.innerText = isSelectedModelAvailable ? "Load" : "Download";
    modelStatus.title = isSelectedModelAvailable
      ? "Load the model to start LLM"
      : "Download the model and start LLM";

    loadModel();
  }
</script>
